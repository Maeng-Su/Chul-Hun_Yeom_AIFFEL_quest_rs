{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRlYgA8/WhHNSkXu9ifea3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paranoidandroid2124/AIFFEL_quest_rs/blob/main/first_trial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XA2Beo9YdZS_"
      },
      "outputs": [],
      "source": [
        "import xgboost\n",
        "import lightgbm\n",
        "from os.path import join\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# 로컬 경로\n",
        "path_temp = \"C:/Users/양자/Desktop/Hun_Works/practices/kagglekr_pricing/data\"\n",
        "train_path = join(path_temp, 'train.csv')\n",
        "test_path = join(path_temp, 'test.csv')\n",
        "\n",
        "# CSV 읽기\n",
        "train = pd.read_csv(train_path)\n",
        "test = pd.read_csv(test_path)\n",
        "\n",
        "train.info()\n",
        "\n",
        "# colum들 중에서 불필요하다고 생각되는 애들을 지워보자\n",
        "## 너무 한 값에 몰려있거나 그런 것들만 제했음\n",
        "del train['id']\n",
        "del train['view']\n",
        "del train['waterfront']\n",
        "del train['yr_renovated']\n",
        "train['date'] = train['date'].apply(lambda i: i[:6]).astype(int)\n",
        "\n",
        "del test['id']\n",
        "del test['view']\n",
        "del test['waterfront']\n",
        "del test['yr_renovated']\n",
        "test['date'] = test['date'].apply(lambda i: i[:6]).astype(int)\n",
        "\n",
        "#=========================================================\n",
        "# (2) Target / Feature 분리\n",
        "#---------------------------------------------------------\n",
        "y = train['price']  # price를 타겟으로\n",
        "del train['price']  # train엔 price 빼준다\n",
        "\n",
        "# 시각화를 잠깐 해보자 (sqft_lot15)\n",
        "plt.figure()\n",
        "sns.countplot(data=train, x='sqft_lot15')\n",
        "plt.title(\"Countplot of sqft_lot15 - 그냥 궁금해서 그려봄\")\n",
        "plt.show()\n",
        "\n",
        "# price 분포도 한 번 보자\n",
        "plt.figure()\n",
        "sns.kdeplot(data=y)\n",
        "plt.title(\"Original Price Distribution\")\n",
        "plt.show()\n",
        "\n",
        "# 로그 변환 (오른쪽 꼬리 긴 애들을 좀 편하게 다루기 위해)\n",
        "y = np.log1p(y)\n",
        "\n",
        "plt.figure()\n",
        "sns.kdeplot(data=y)\n",
        "plt.title(\"Log1p Transformed Price Distribution\")\n",
        "plt.show()\n",
        "\n",
        "#=========================================================\n",
        "# (3) 모델 평가 함수 & 모델 리스트\n",
        "#---------------------------------------------------------\n",
        "def rmse(y_test, y_pred):\n",
        "    \"\"\"\n",
        "    RMSE 함수 정의.\n",
        "    음... 사실 sqrt(mse) 해주는 거지.\n",
        "    \"\"\"\n",
        "    return np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "random_state = 2025\n",
        "\n",
        "# 주요 모델들\n",
        "gboost = GradientBoostingRegressor(random_state=random_state)\n",
        "xgboost_model = XGBRegressor(random_state=random_state)\n",
        "lightgbm_model = LGBMRegressor(random_state=random_state)\n",
        "rdforest = RandomForestRegressor(random_state=random_state)\n",
        "\n",
        "models = [gboost, xgboost_model, lightgbm_model, rdforest]\n",
        "\n",
        "def get_scores(models, train, y):\n",
        "    \"\"\"\n",
        "    여러 모델에 대해 동일한 train_test_split으로 나눠서\n",
        "    fit -> predict -> rmse 계산해 주는 간단한 함수.\n",
        "    \"\"\"\n",
        "    df = {}\n",
        "    for model in models:\n",
        "        model_name = model.__class__.__name__\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            train, y,\n",
        "            random_state=random_state,\n",
        "            test_size=0.2\n",
        "        )\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        df[model_name] = rmse(y_test, y_pred)\n",
        "    score_df = pd.DataFrame(df, index=['RMSE']).T.sort_values('RMSE', ascending=False)\n",
        "    return score_df\n",
        "\n",
        "score_result = get_scores(models, train, y)\n",
        "print(\"초기 모델별 RMSE:\\n\", score_result)\n",
        "\n",
        "#=========================================================\n",
        "# (4) GridSearch로 하이퍼파라미터 탐색 (샘플)\n",
        "#---------------------------------------------------------\n",
        "def my_GridSearch(model, train, y, param_grid, verbose=2, n_jobs=5):\n",
        "    \"\"\"\n",
        "    GridSearchCV를 활용해서 파라미터 튜닝을 도와주는 함수.\n",
        "    scoring은 MSE에 -를 붙인 neg_mean_squared_error 써서,\n",
        "    RMSLE 계산하려면 sqrt(-score) 해줘야 함!\n",
        "    \"\"\"\n",
        "    grid_model = GridSearchCV(\n",
        "        model,\n",
        "        param_grid=param_grid,\n",
        "        scoring='neg_mean_squared_error',\n",
        "        cv=5,\n",
        "        verbose=verbose,\n",
        "        n_jobs=n_jobs\n",
        "    )\n",
        "    grid_model.fit(train, y)\n",
        "\n",
        "    # 결과를 예쁘게 DataFrame으로 뽑아주자\n",
        "    params = grid_model.cv_results_['params']\n",
        "    score = grid_model.cv_results_['mean_test_score']\n",
        "    score_df = pd.DataFrame(score, columns=['score'])\n",
        "    results = pd.DataFrame(params)\n",
        "    results['score'] = score_df\n",
        "    results['RMSLE'] = np.sqrt(-1 * results['score'])\n",
        "    results = results.sort_values('RMSLE')\n",
        "    return results\n",
        "\n",
        "# GBoost 예시\n",
        "model = gboost\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.2],\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [3, 5, 10],\n",
        "    'min_samples_split': [5, 10],\n",
        "    'min_samples_leaf': [5, 10],\n",
        "}\n",
        "ans1 = my_GridSearch(model, train, y, param_grid)\n",
        "print(\"GradientBoosting GridSearch 결과 (상위 1개):\\n\", ans1.iloc[0])\n",
        "\n",
        "#=========================================================\n",
        "# (5) LightGBM, XGBoost도 잠깐만 탐색\n",
        "#---------------------------------------------------------\n",
        "model = lightgbm_model\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [-1, 5],\n",
        "    'num_leaves': [15, 31],\n",
        "}\n",
        "ans2 = my_GridSearch(model, train, y, param_grid)\n",
        "print(\"LightGBM GridSearch 결과 (상위 1개):\\n\", ans2.iloc[0])\n",
        "\n",
        "model = xgboost_model\n",
        "param_grid = {\n",
        "    'n_estimators': [10, 50, 200],\n",
        "    'max_depth': [20],\n",
        "    'min_samples_split': [2, 10],\n",
        "    'min_samples_leaf': [1, 4],\n",
        "    'max_features': [0.75, 1.0],\n",
        "}\n",
        "ans3 = my_GridSearch(model, train, y, param_grid)\n",
        "print(\"XGBoost GridSearch 결과 (상위 1개):\\n\", ans3.iloc[0])\n",
        "\n",
        "#=========================================================\n",
        "# (6) 최적 파라미터로 예측해보기\n",
        "#---------------------------------------------------------\n",
        "\"\"\"\n",
        "GridSearch 결과를 바탕으로 각 모델에 파라미터를 세팅해서 fit 후,\n",
        "test 데이터를 predict하자. 그리고 np.expm1으로 되돌린다(로그변환을 풀어주는 거).\n",
        "\"\"\"\n",
        "# GradientBoosting\n",
        "model = GradientBoostingRegressor(\n",
        "    learning_rate=0.2,\n",
        "    max_depth=10,\n",
        "    min_samples_leaf=5,\n",
        "    min_samples_split=10,\n",
        "    n_estimators=50,\n",
        "    random_state=2025\n",
        ")\n",
        "model.fit(train, y)\n",
        "y_pred1 = model.predict(test)\n",
        "y_pred1 = np.expm1(y_pred1)\n",
        "\n",
        "# LightGBM\n",
        "model = LGBMRegressor(\n",
        "    learning_rate=0.1,\n",
        "    max_depth=5,\n",
        "    n_estimators=100,\n",
        "    num_leaves=31,\n",
        "    random_state=2025\n",
        ")\n",
        "model.fit(train, y)\n",
        "y_pred2 = model.predict(test)\n",
        "y_pred2 = np.expm1(y_pred2)\n",
        "\n",
        "# XGBoost\n",
        "model = XGBRegressor(\n",
        "    max_depth=20,\n",
        "    colsample_bytree=1.0,\n",
        "    min_child_weight=4,\n",
        "    gamma=10,\n",
        "    n_estimators=200,\n",
        "    random_state=2025\n",
        ")\n",
        "model.fit(train, y)\n",
        "y_pred3 = model.predict(test)\n",
        "y_pred3 = np.expm1(y_pred3)\n",
        "\n",
        "#=========================================================\n",
        "# (7) 모델 스태킹(Blending) 예시\n",
        "#---------------------------------------------------------\n",
        "\"\"\"\n",
        "조금 더 복잡해 보이는 부분. Blending해서 최종 예측을 하려는 것.\n",
        "'메타 모델'로 Ridge를 쓴다.\n",
        "\"\"\"\n",
        "def stack_and_predict(models, train, y, test):\n",
        "    \"\"\"\n",
        "    1) train/val 나누기\n",
        "    2) 각 모델로 학습 후 validation, test 예측\n",
        "    3) 예측 결과를 합쳐서 meta_X_train, meta_X_test 생성\n",
        "    4) Ridge로 학습 (meta_model)\n",
        "    5) 최종 예측 -> expm1으로 로그 변환 풀기\n",
        "    \"\"\"\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        train, y,\n",
        "        test_size=0.2,\n",
        "        random_state=2025\n",
        "    )\n",
        "\n",
        "    # 일단 개별 모델들 학습\n",
        "    for m in models:\n",
        "        m.fit(X_train, y_train)\n",
        "\n",
        "    # validation, test 예측\n",
        "    val_preds = [m.predict(X_val) for m in models]\n",
        "    test_preds = [m.predict(test) for m in models]\n",
        "\n",
        "    # column_stack으로 합침\n",
        "    meta_X_train = np.column_stack(val_preds)\n",
        "    meta_X_test = np.column_stack(test_preds)\n",
        "\n",
        "    # 메타 모델은 Ridge\n",
        "    meta_model = Ridge()\n",
        "    meta_model.fit(meta_X_train, y_val)\n",
        "\n",
        "    # 최종 예측(로그 -> exp 변환)\n",
        "    meta_y_test_pred = meta_model.predict(meta_X_test)\n",
        "    final_prediction = np.expm1(meta_y_test_pred)\n",
        "    return final_prediction\n",
        "\n",
        "model1 = GradientBoostingRegressor(\n",
        "    learning_rate=0.2,\n",
        "    max_depth=10,\n",
        "    min_samples_leaf=5,\n",
        "    min_samples_split=10,\n",
        "    n_estimators=50,\n",
        "    random_state=2025\n",
        ")\n",
        "model2 = LGBMRegressor(\n",
        "    learning_rate=0.1,\n",
        "    max_depth=5,\n",
        "    n_estimators=100,\n",
        "    num_leaves=31,\n",
        "    random_state=2025\n",
        ")\n",
        "model3 = XGBRegressor(\n",
        "    max_depth=20,\n",
        "    colsample_bytree=1.0,\n",
        "    min_child_weight=4,\n",
        "    gamma=10,\n",
        "    n_estimators=200,\n",
        "    random_state=2025\n",
        ")\n",
        "\n",
        "final_prediction = stack_and_predict([model1, model2, model3], train, y, test)\n",
        "\n",
        "#=========================================================\n",
        "# (8) 최종 예측 결과 저장\n",
        "#---------------------------------------------------------\n",
        "\n",
        "submission_path = join(path_temp, 'sample_submission.csv')\n",
        "submission = pd.read_csv(submission_path)\n",
        "submission['price'] = final_prediction\n",
        "submission_csv_path = join(path_temp, 'submission.csv')\n",
        "submission.to_csv(submission_csv_path, index=False)\n",
        "\n",
        "print(\"[PRT 1] 최종 예측 결과를 여기다 저장했음:\", submission_csv_path)\n",
        "\n",
        "#=========================================================\n",
        "# (9) 회고(Reflection) & 실행 플로우 간단 설명\n",
        "#---------------------------------------------------------\n",
        "\"\"\"\n",
        "[회고(Reflection)]\n",
        "- 캐글 평가수치 13.9만 에잉 ..data 전처리할 때 95% 동일값이던 열들을 다 버렸는데\n",
        "  사실 더 자세히 EDA해서 의미 있는 피처가 있는지 확인했으면 좋았을 것 같다.\n",
        "- 로그 변환해서 타겟 분포가 좀 편해진 건 괜찮았는데,\n",
        "  expm1으로 되돌릴 때 굉장히 큰 값이 튀어나올 수도 있어서\n",
        "  그 부분은 조심해야 한다.\n",
        "- 모델 스태킹의 메타 모델로 Ridge만 썼는데, 다른 것도 시도해보면 좋다.\n",
        "- GridSearch 파라미터 후보 많으면 진짜 시간 오래 걸린다.\n",
        "  나중엔 Bayesian Optimization 같은 것도 고려해볼 수 있겠지.\n",
        "\n",
        "[실행 플로우 그림(텍스트 대체)]\n",
        "(1) CSV 로드 -> (2) 불필요 열 제거, date 변환 -> (3) 타겟 로그 변환 -> (4) 다양한 모델 fit + RMSE\n",
        "-> (5) GridSearch로 하이퍼파라미터 탐색 -> (6) 최적 모델로 test 예측 -> (7) 스태킹 시도 -> (8) 최종 CSV 저장\n",
        "여기까지가 전체 플로우!\n",
        "\n",
        "[디버깅 기록]\n",
        "- XGB에서 min_samples_split, min_samples_leaf를 쓰면 에러 났었음.\n",
        "  XGBoost는 별도 파라미터(min_child_weight 같은) 써야 하더라고.\n",
        "- learning_rate를 0.2로 했더니 가끔 과적합이 우려되긴 함.\n",
        "  파라미터 더 줄여가며 교차검증을 좀 더 꼼꼼히 해야 할 듯.\n",
        "\"\"\"\n"
      ]
    }
  ]
}
