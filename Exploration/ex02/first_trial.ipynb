{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMiUPk1MFZ3brle3KJV0JZk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paranoidandroid2124/AIFFEL_quest_rs/blob/main/first_trial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdJhshk1Qmu7"
      },
      "outputs": [],
      "source": [
        "import xgboost\n",
        "import lightgbm\n",
        "from os.path import join\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#%%\n",
        "pd.reset_option('display.max_rows')\n",
        "pd.reset_option('display.max_columns')\n",
        "pd.reset_option('display.width')\n",
        "pd.reset_option('display.max_colwidth')\n",
        "\n",
        "#%%\n",
        "#로컬에서 작업해서 로컬 폴더로 주소 지정\n",
        "path_temp = \"C:/Users/양자/Desktop/Hun_Works/practices/kagglekr_pricing/data\"\n",
        "train_path = join(path_temp, 'train.csv')\n",
        "test_path = join(path_temp, 'test.csv')\n",
        "train = pd.read_csv(train_path)\n",
        "test = pd.read_csv(test_path)\n",
        "#%%\n",
        "train.columns\n",
        "train.info()"
      ],
      "metadata": {
        "id": "DH3nJbEAQrgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%\n",
        "## id는 쓸모없어서, 나머지는 95% 이상의 데이터가 한 값을 갖고 있어 편향 줄까봐 제거\n",
        "del train['id']\n",
        "del train['view']\n",
        "del train['waterfront']\n",
        "del train['yr_renovated']\n",
        "train['date'] = train['date'].apply(lambda i: i[:6]).astype(int)\n",
        "\n",
        "## 테스트도 마찬가지로 트레인 셋과 열 동기화\n",
        "del test['id']\n",
        "del test['view']\n",
        "del test['waterfront']\n",
        "del test['yr_renovated']\n",
        "test['date'] = test['date'].apply(lambda i: i[:6]).astype(int)\n",
        "\n",
        "#%%\n",
        "y = train['price']\n",
        "del train['price']\n",
        "## 나중에 꼭 바꿔줄 것\n",
        "y = np.log1p(y)"
      ],
      "metadata": {
        "id": "X2bSnBMBQwFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%\n",
        "plt.figure()\n",
        "sns.countplot(data=train, x='sqft_lot15')\n",
        "plt.show()\n",
        "#%%\n",
        "plt.figure()\n",
        "sns.kdeplot(data=y)\n",
        "plt.show()\n",
        "#%%\n",
        "plt.figure()\n",
        "sns.kdeplot(data=y)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IRG9j7yxQ0dA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%\n",
        "## rmse 정의\n",
        "def rmse(y_test,y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_test, y_pred))"
      ],
      "metadata": {
        "id": "u0huX-SbRHjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%\n",
        "# initial model 정의하기\n",
        "random_state = 2025\n",
        "\n",
        "gboost = GradientBoostingRegressor(random_state=random_state)\n",
        "xgboost = XGBRegressor(random_state=random_state)\n",
        "lightgbm = LGBMRegressor(random_state=random_state)\n",
        "rdforest = RandomForestRegressor(random_state=random_state)\n",
        "\n",
        "models = [gboost, xgboost, lightgbm, rdforest]"
      ],
      "metadata": {
        "id": "u0Nl-8BSRKFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 노드에서 사용한 함수 재정의해서 사용\n",
        "def get_scores(models, train, y):\n",
        "    df = {}\n",
        "\n",
        "    for model in models:\n",
        "        model_name = model.__class__.__name__\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(train, y, random_state=random_state, test_size=0.2)\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        df[model_name] = rmse(y_test, y_pred)\n",
        "    score_df = pd.DataFrame(df, index=['RMSE']).T.sort_values('RMSE', ascending=False)\n",
        "\n",
        "    return score_df\n",
        "#%%\n",
        "get_scores(models, train, y)"
      ],
      "metadata": {
        "id": "ENt_YVSHRS2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%\n",
        "def my_GridSearch(model, train, y, param_grid, verbose=2, n_jobs=5):\n",
        "    grid_model = GridSearchCV(model, param_grid=param_grid, \\\n",
        "                        scoring='neg_mean_squared_error', \\\n",
        "                        cv=5, verbose=verbose, n_jobs=n_jobs)\n",
        "    grid_model.fit(train, y)\n",
        "    params = grid_model.cv_results_['params']\n",
        "    score = grid_model.cv_results_['mean_test_score']\n",
        "    score=pd.DataFrame(data = score, columns = ['score'])\n",
        "    results = pd.DataFrame(params)\n",
        "    results['score'] = score\n",
        "    results['RMSLE'] = np.sqrt(-1 * results['score'])\n",
        "    results = results.sort_values('RMSLE')\n",
        "    return results\n",
        "\n",
        "#%%\n",
        "## 대강의 결과\n",
        "##                               RMSE\n",
        "##XGBRegressor               0.025468\n",
        "##LGBMRegressor              0.019407\n",
        "##RandomForestRegressor      0.006283\n",
        "##GradientBoostingRegressor  0.005782"
      ],
      "metadata": {
        "id": "QhiooSJbRbVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = gboost\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.2],\n",
        "    'n_estimators': [50, 100], # 트리 개수\n",
        "    'max_depth': [3, 5, 10], # 깊이\n",
        "    'min_samples_split': [5, 10], # 노드 분할 최소 샘플 수\n",
        "    'min_samples_leaf': [5, 10], # 리프 노드 최소 샘플 수\n",
        "}\n",
        "ans1 = my_GridSearch(model, train, y, param_grid)\n",
        "## 원랜 더 많이 해서 144 개 candidate x 5 folds = 720번 = 시간 진짜 하루종일 걸림.. 줄여봤음\n",
        "## best:\n",
        "## learning_rate         0.200000\n",
        "## max_depth            10.000000\n",
        "## min_samples_leaf      5.000000\n",
        "## min_samples_split    10.000000\n",
        "## n_estimators         50.000000\n",
        "## score                -0.000014\n",
        "## RMSLE                 0.003742: 시작부터 y를 로그스케일로 만들고 했음. 마지막에 다시 exponential 해주면될듯?\n",
        "#%%\n",
        "## lightgbm 할 때는 가능한 candidate 개수를 확 줄여보자\n",
        "model = lightgbm\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.1],         # 학습 속도\n",
        "    'n_estimators': [50, 100],                 # 트리 개수\n",
        "    'max_depth': [-1, 5],                   # 깊이\n",
        "    'num_leaves': [15, 31],                 # 리프 노드 개수\n",
        "}\n",
        "ans2 = my_GridSearch(model, train, y, param_grid)\n",
        "## best:\n",
        "## learning_rate      0.100000\n",
        "## max_depth          5.000000\n",
        "## n_estimators     100.000000\n",
        "## num_leaves        31.000000\n",
        "## score             -0.000225\n",
        "## RMSLE              0.015004\n",
        "#%%\n",
        "model = xgboost\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],  # 트리 개수\n",
        "    'max_depth': [None, 20],    # 트리 최대 깊이\n",
        "    'min_samples_split': [2, 10],    # 노드 분할 최소 샘플 수\n",
        "    'min_samples_leaf': [1, 4],   # 리프 노드 최소 샘플 수\n",
        "    'max_features': [0.75, 1.0],       # 최대 피처 비율\n",
        "}\n",
        "ans3 = my_GridSearch(model, train, y, param_grid)\n",
        "## best:\n",
        "## max_depth                20.0\n",
        "## max_features              1.0\n",
        "## min_samples_leaf            4\n",
        "## min_samples_split          10\n",
        "## n_estimators              200\n",
        "## score               -0.000357\n",
        "## RMSLE                0.018891\n",
        "#%%"
      ],
      "metadata": {
        "id": "KFg5LDugRiJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 아까 그리드서치로 얻은 하이퍼파라미터 대입\n",
        "model = GradientBoostingRegressor(learning_rate=0.2, max_depth=10, min_samples_leaf=5, min_samples_split=10, n_estimators=50)\n",
        "model.fit(train,y)\n",
        "y_pred1 = model.predict(test)\n",
        "y_pred1 = np.expm1(y_pred1)\n",
        "#%%\n",
        "model = LGBMRegressor(learning_rate=0.1, max_depth=5, n_estimators=100, num_leaves=31)\n",
        "model.fit(train,y)\n",
        "y_pred2 = model.predict(test)\n",
        "y_pred2 = np.expm1(y_pred2)\n",
        "#%%\n",
        "model = XGBRegressor(\n",
        "    max_depth=20,\n",
        "    colsample_bytree=1.0,\n",
        "    min_child_weight=4,\n",
        "    gamma=10,\n",
        "    n_estimators=200\n",
        ")\n",
        "model.fit(train, y)\n",
        "y_pred3 = model.predict(test)\n",
        "y_pred3 = np.expm1(y_pred3)\n",
        "\n",
        "model.fit(train,y)\n",
        "y_pred3 = model.predict(test)\n",
        "y_pred3 = np.expm1(y_pred3)"
      ],
      "metadata": {
        "id": "mjQe4x7qRmNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%\n",
        "from sklearn.linear_model import Ridge # L2 정칙화 사용\n",
        "#스태킹 사용할 것.\n",
        "# 1) train/val 분할\n",
        "X_train, X_val, y_train, y_val = train_test_split(train, y, test_size=0.2, random_state=2025)\n",
        "\n",
        "# 2) 개별 모델 학습 및 예측\n",
        "model1 = GradientBoostingRegressor(\n",
        "    learning_rate=0.2,\n",
        "    max_depth=10,\n",
        "    min_samples_leaf=5,\n",
        "    min_samples_split=10,\n",
        "    n_estimators=50,\n",
        "    random_state=2025\n",
        ")\n",
        "model2 = LGBMRegressor(\n",
        "    learning_rate=0.1,\n",
        "    max_depth=5,\n",
        "    n_estimators=100,\n",
        "    num_leaves=31,\n",
        "    random_state=2025\n",
        ")\n",
        "model3 = XGBRegressor(\n",
        "    max_depth=20,\n",
        "    colsample_bytree=1.0,\n",
        "    min_child_weight=4,\n",
        "    gamma=10,\n",
        "    n_estimators=200,\n",
        "    random_state=2025\n",
        ")\n",
        "\n",
        "models = [model1, model2, model3]\n",
        "\n",
        "# 각각 학습\n",
        "for m in models:\n",
        "    m.fit(X_train, y_train)\n",
        "\n",
        "# validation 예측\n",
        "val_pred1 = model1.predict(X_val)\n",
        "val_pred2 = model2.predict(X_val)\n",
        "val_pred3 = model3.predict(X_val)\n",
        "\n",
        "# test 예측\n",
        "test_pred1 = model1.predict(test)\n",
        "test_pred2 = model2.predict(test)\n",
        "test_pred3 = model3.predict(test)\n",
        "\n",
        "# 3) 스택 데이터 생성 (validation, test 모두)\n",
        "meta_X_train = np.column_stack((val_pred1, val_pred2, val_pred3))\n",
        "meta_X_test  = np.column_stack((test_pred1, test_pred2, test_pred3))\n",
        "\n",
        "# 4) 메타 모델 학습\n",
        "meta_model = Ridge()\n",
        "meta_model.fit(meta_X_train, y_val)\n",
        "\n",
        "# 5) 최종 예측: log scale → 원래 스케일 역변환\n",
        "meta_y_test_pred = meta_model.predict(meta_X_test)\n",
        "final_prediction = np.expm1(meta_y_test_pred)\n",
        "\n",
        "# 최종 예측 결과: final_prediction\n",
        "\n",
        "submission_path = join('C:/Users/양자/Desktop/Hun_Works/practices/kagglekr_pricing/data', 'sample_submission.csv')\n",
        "submission = pd.read_csv(submission_path)\n",
        "submission['price'] = final_prediction\n",
        "\n",
        "submission_csv_path = '{}/submission.csv'.format('C:/Users/양자/Desktop/Hun_Works/practices/kagglekr_pricing/data')\n",
        "submission.to_csv(submission_csv_path, index=False)\n",
        "print(submission_csv_path)\n",
        "\n",
        "# 결과 스코어 값: 139314: 망했음"
      ],
      "metadata": {
        "id": "MLkC4fKiRA0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aI6P-F5NRImn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
